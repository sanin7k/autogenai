# ğŸ¤– AutogenAI

**Unified, pluggable SDK for LLMs â€” OpenAI, Gemini, and more.**  
Production-grade interface for integrating and abstracting multiple LLM providers via a clean Python API.

---

## ğŸš€ Features

- âœ… Easy-to-use `chat()` interface for multiple LLMs
- âœ… Supports **OpenAI** and **Gemini** out of the box
- âœ… Built-in error handling (`LLMResponseError`)
- âœ… Easily extendable for other providers (Claude, Mistral, Ollama, etc.)
- âœ… Semantic versioning & PyPI-ready

---

## ğŸ“¦ Installation

From PyPI (once released):

```bash
pip install autogenai
From TestPyPI (for testing pre-releases):

bash
Copy code
pip install -i https://test.pypi.org/simple/ autogenai
From GitHub (dev version):

bash
Copy code
pip install git+https://github.com/yourusername/autogenai.git
ğŸ§  Quick Start
âœ… OpenAI Example
python
Copy code
from autogenai import OpenAIEngine

engine = OpenAIEngine(api_key="your-openai-api-key")
response = engine.chat("Tell me a joke.")
print(response)
âœ… Gemini Example
python
Copy code
from autogenai import GeminiEngine

engine = GeminiEngine(api_key="your-gemini-api-key")
response = engine.chat("What's the capital of Japan?")
print(response)
ğŸ§ª Advanced Usage
âœ… Injecting custom config
python
Copy code
from autogenai import OpenAIEngine

engine = OpenAIEngine(
    api_key="your-openai-api-key",
    model="gpt-4",
    system_prompt="You are a helpful assistant.",
)
âœ… Error Handling
python
Copy code
from autogenai.utils.errors import LLMResponseError

try:
    engine.chat("Hello!")
except LLMResponseError as e:
    print(f"Request failed: {e}")
    print("Status Code:", e.status_code)
    print("Response Data:", e.response_data)
ğŸ§© Extending to Other LLMs
You can subclass BaseEngine to support any provider:

python
Copy code
from autogenai.core.base import BaseEngine

class MyLLMEngine(BaseEngine):
    def chat(self, prompt: str) -> str:
        # Implement your own logic here
        ...
ğŸ”§ Project Structure
bash
Copy code
autogenai/
â”œâ”€â”€ core/           # LLM engines (OpenAI, Gemini)
â”œâ”€â”€ utils/          # Error classes, config loading
â”œâ”€â”€ examples/       # Usage demos
â”œâ”€â”€ tests/          # Unit tests

ğŸ“œ License
This project is licensed under the MIT License.

ğŸ‘¨â€ğŸ’» Author
Built with â¤ï¸ by Sanin K

ğŸ§­ Roadmap
 CLI support (autogenai chat "hi")

 Add Claude / Mistral / Ollama adapters

 Async support with httpx.AsyncClient

 Prompt templating

 Streaming & function calling support

ğŸ“¦ Changelog
See CHANGELOG.md for version history.

ğŸ› Contributing
Pull requests are welcome! Please open an issue first to discuss major changes.

â­ Star if you like it!
If you find this useful, please star â­ the repo. It helps others discover it too!
